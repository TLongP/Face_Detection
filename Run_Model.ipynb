{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from preprocessing import combine_data_frompath\n",
    "import datetime\n",
    "from tensorflow.keras import callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this code below to see that you are using GPU for computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = combine_data_frompath('aug_data\\\\train\\\\labels\\\\*.json','aug_data\\\\train\\\\images\\\\*.jpg')\n",
    "val_data = combine_data_frompath('aug_data\\\\val\\\\labels\\\\*.json','aug_data\\\\val\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FaceTracker\n",
    "facetracker = FaceTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train_data)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n",
    "facetracker.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the tensorboard we will plot the training loss in tensorboard\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"logs/training/\" + time \n",
    "tf_board_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stopping_callback = callbacks.EarlyStopping(monitor=\"val_total_loss\",patience=3)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 26s 475ms/step - total_loss: 2.5377 - classloss: 0.3023 - coordloss: 2.2355 - val_total_loss: 0.2248 - val_classloss: 0.0185 - val_coordloss: 0.2063\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 14s 392ms/step - total_loss: 0.3482 - classloss: 0.0103 - coordloss: 0.3379 - val_total_loss: 0.0291 - val_classloss: 9.0777e-05 - val_coordloss: 0.0290\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 14s 392ms/step - total_loss: 0.0618 - classloss: 2.8780e-04 - coordloss: 0.0615 - val_total_loss: 0.0107 - val_classloss: 2.8360e-05 - val_coordloss: 0.0107\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 14s 391ms/step - total_loss: 0.0361 - classloss: 1.6228e-04 - coordloss: 0.0359 - val_total_loss: 0.0100 - val_classloss: 1.6306e-05 - val_coordloss: 0.0100\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 14s 389ms/step - total_loss: 0.0291 - classloss: 9.7596e-05 - coordloss: 0.0290 - val_total_loss: 0.0078 - val_classloss: 1.2666e-05 - val_coordloss: 0.0077\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 14s 389ms/step - total_loss: 0.0263 - classloss: 8.3045e-05 - coordloss: 0.0263 - val_total_loss: 0.0138 - val_classloss: 1.4722e-05 - val_coordloss: 0.0138\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 14s 390ms/step - total_loss: 0.0217 - classloss: 6.6942e-05 - coordloss: 0.0217 - val_total_loss: 0.0083 - val_classloss: 9.6475e-06 - val_coordloss: 0.0082\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 14s 391ms/step - total_loss: 0.0159 - classloss: 5.3655e-05 - coordloss: 0.0158 - val_total_loss: 0.0120 - val_classloss: 8.1872e-06 - val_coordloss: 0.0119\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 14s 390ms/step - total_loss: 0.0181 - classloss: 4.5918e-05 - coordloss: 0.0180 - val_total_loss: 0.0151 - val_classloss: 8.0126e-06 - val_coordloss: 0.0151\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 15s 393ms/step - total_loss: 0.0165 - classloss: 4.4263e-05 - coordloss: 0.0164 - val_total_loss: 0.0106 - val_classloss: 5.8540e-06 - val_coordloss: 0.0106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee91def970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facetracker.fit(train_data,epochs=num_epochs,\n",
    "                callbacks=[\n",
    "                            tf_board_callback,\n",
    "                            early_stopping_callback],\n",
    "                validation_data=val_data,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d72d12c0011cf85239c4aa10809faba480ed361e10dd0e571e7374550f87005"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
