{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from preprocessing import combine_data_frompath\n",
    "import datetime\n",
    "from tensorflow.keras import callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this code below to see that you are using GPU for computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = combine_data_frompath('aug_data\\\\train\\\\labels\\\\*.json','aug_data\\\\train\\\\images\\\\*.jpg',batch_size=16)\n",
    "val_data = combine_data_frompath('aug_data\\\\val\\\\labels\\\\*.json','aug_data\\\\val\\\\images\\\\*.jpg',batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FaceTracker\n",
    "facetracker = FaceTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2b5ae835fa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facetracker.load_weights(\"facetracker\\\\00001\") # load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train_data)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n",
    "facetracker.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the tensorboard we will plot the training loss in tensorboard\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"logs/training/\" + time \n",
    "tf_board_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stopping_callback = callbacks.EarlyStopping(monitor=\"val_total_loss\",patience=3)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "74/74 [==============================] - 23s 171ms/step - total_loss: 1.3910 - classloss: 0.0368 - coordloss: 1.3542 - val_total_loss: 0.2240 - val_classloss: 0.0043 - val_coordloss: 0.2198\n",
      "Epoch 2/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.1551 - classloss: 0.0018 - coordloss: 0.1533 - val_total_loss: 0.0710 - val_classloss: 0.0014 - val_coordloss: 0.0696\n",
      "Epoch 3/100\n",
      "74/74 [==============================] - 11s 143ms/step - total_loss: 0.0744 - classloss: 9.5405e-04 - coordloss: 0.0734 - val_total_loss: 0.0586 - val_classloss: 8.2422e-04 - val_coordloss: 0.0578\n",
      "Epoch 4/100\n",
      "74/74 [==============================] - 11s 145ms/step - total_loss: 0.0562 - classloss: 6.4133e-04 - coordloss: 0.0556 - val_total_loss: 0.0532 - val_classloss: 5.6539e-04 - val_coordloss: 0.0526\n",
      "Epoch 5/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0567 - classloss: 4.7763e-04 - coordloss: 0.0562 - val_total_loss: 0.0389 - val_classloss: 4.2643e-04 - val_coordloss: 0.0385\n",
      "Epoch 6/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0469 - classloss: 3.7711e-04 - coordloss: 0.0466 - val_total_loss: 0.0282 - val_classloss: 3.4174e-04 - val_coordloss: 0.0278\n",
      "Epoch 7/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0377 - classloss: 3.0953e-04 - coordloss: 0.0374 - val_total_loss: 0.0371 - val_classloss: 2.8220e-04 - val_coordloss: 0.0368\n",
      "Epoch 8/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0281 - classloss: 2.6189e-04 - coordloss: 0.0279 - val_total_loss: 0.0299 - val_classloss: 2.4036e-04 - val_coordloss: 0.0296\n",
      "Epoch 9/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0244 - classloss: 2.2682e-04 - coordloss: 0.0242 - val_total_loss: 0.0275 - val_classloss: 2.0954e-04 - val_coordloss: 0.0273\n",
      "Epoch 10/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0217 - classloss: 2.0002e-04 - coordloss: 0.0215 - val_total_loss: 0.0343 - val_classloss: 1.8582e-04 - val_coordloss: 0.0341\n",
      "Epoch 11/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0206 - classloss: 1.7884e-04 - coordloss: 0.0204 - val_total_loss: 0.0359 - val_classloss: 1.6671e-04 - val_coordloss: 0.0357\n",
      "Epoch 12/100\n",
      "74/74 [==============================] - 11s 144ms/step - total_loss: 0.0186 - classloss: 1.6170e-04 - coordloss: 0.0184 - val_total_loss: 0.0316 - val_classloss: 1.5118e-04 - val_coordloss: 0.0315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25627304bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facetracker.fit(\n",
    "                train_data,epochs=num_epochs,\n",
    "                callbacks=[\n",
    "                            tf_board_callback,\n",
    "                            early_stopping_callback\n",
    "                            ],\n",
    "                validation_data=val_data,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: facetracker\\00001\\assets\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "path = \"facetracker\"\n",
    "version = \"00001\"\n",
    "facetracker.save(filepath=os.path.join(path,version),save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x256bf72d970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facetracker.load_weights(\"facetracker\\\\00001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facetracker\"\n",
    "for root, dirs, files in os.walk(model_name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    for filename in files:\n",
    "        print('{}{}'.format(indent + '    ', filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first install docker then \n",
    "docker pull tensorflow/serving\n",
    "then run the following command\n",
    "\n",
    "docker run -p 8501:8501 --name tfserving_facetracker --mount type=bind,source=[model_source],target=/models/facetracker -e MODEL_NAME=facetracker -t tensorflow/serving\n",
    "\n",
    "note that model_source is the the location where you save your model for example\n",
    "\n",
    "source = \"...\\facetracker\\\"\n",
    "... denotes the path to the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "url = 'http://localhost:8501/v1/models/facetracker:predict'\n",
    "\n",
    "def preprocess_input(image):\n",
    "   img = cv2.resize(image,[120,120])/255.0\n",
    "   return img[np.newaxis]\n",
    "\n",
    "def make_prediction(instances):\n",
    "   img = preprocess_input(instances)\n",
    "   data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": img.tolist()})\n",
    "   headers = {\"content-type\": \"application/json\"}\n",
    "   json_response = requests.post(url, data=data, headers=headers)\n",
    "   predictions = json.loads(json_response.text)['predictions']\n",
    "   return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    \n",
    "    yhat = make_prediction(rgb)\n",
    "    sample_coords = yhat[0][\"output_2\"]\n",
    "    \n",
    "    if yhat[0][\"output_1\"][0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(yhat)\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36a2c5e48267cbd778fdc39791df5b942afcfbc53edd612b1e0033fc48c00af0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
